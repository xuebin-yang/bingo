<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
    <title>Post1 - yangxuebin</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"> 
    <meta name="description" content="杨学彬的个人网站">
    <meta name="generator" content="mkdocs-1.6.1, mkdocs-gitbook-1.0.7"> 
    <meta name="author" content="杨学彬">
    <link rel="shortcut icon" href="./img/headshot.jpg" type="image/x-icon">
    <meta name="HandheldFriendly" content="true" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta rel="next" href="" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&display=swap">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;600;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+Pro:wght@300;400;600&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Lora:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Raleway:wght@300;400;500;600&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300;400;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/octicons/4.4.0/font/octicons.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Liu+Jian+Mao+Cao&display=swap" rel="stylesheet">

    <link href="../css/style.min.css" rel="stylesheet"><link href="../stylesheets/extra.css" rel="stylesheet"><link href="../stylesheets/index.css" rel="stylesheet"><link href="../stylesheets/about_me.css" rel="stylesheet"><link href="../stylesheets/my_posts.css" rel="stylesheet"><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/monokai.min.css" rel="stylesheet"><link href="../stylesheets/recode_life.css" rel="stylesheet"> 
</head>

<body>
    <div class="book">
        <div class="book-summary">
            <!-- Headshot -->
            <div class="sidebar-headshot" style="text-align: center; padding: 30px;">
                <img src="../img/headshot.jpg" alt="Headshot" class="headshot-img">
                <!-- Nickname -->
            </div>
            <div class="sidebar-nickname" style="text-align: center; font-weight: bold; font-size: 18px; padding: 5px;">不能说的秘密</div>
            
            <!-- Navigation --> <nav role="navigation">
    <ul class="summary">
        
        <li class="divider"></li>
                <li class="chapter" data-path="index.html">
                <a href="../index.html">Home</a>
                <li class="chapter" data-path="about_me.html">
                <a href="../about_me.html">About Me</a>
                <li class="chapter" data-path="my_posts.html">
                <a href="../my_posts.html">My Posts</a>
                <li class="chapter" data-path="record_life.html">
                <a href="../record_life.html">Life Recording</a>
            <li class="divider"></li> 
    
    </ul>
</nav>
            <div id="book-search-input" role="search">
                <input type="text" placeholder="search function to be developed" />
            </div>
            <!-- end of book-search-input -->
        </div>
        <!-- end of book-summary -->

        <div class="book-body">
            <div class="body-inner">
                <div class="book-header" role="navigation">
                    <!-- Title -->
                    <h1>
                        <i class="fa fa-circle-o-notch"></i>
                        <a href=".">Post1</a>
                    </h1>
                </div>
                <!-- end of book-header -->

                <div class="page-wrapper" tabindex="-1" role="main">
                    <div class="page-inner"> 

 


<p><font color=ff00ff>前言：扩散模型的推导过程以及基础概率论知识</font></p>
<h1 id="_1">概率论基础</h1>
<h3 id="1-px">1、概率 p(x)</h3>
<p>事件A的概率是对事件A在试验中出现的可能性大小的一种度量，事件A的概率表示为<span class="arithmatex">\(p(A)\)</span></p>
<h3 id="2">2、概率分布</h3>
<p>反映随机变量<span class="arithmatex">\(X\)</span>（可能是离散型、也可能是连续型）的所有可能取值（如<span class="arithmatex">\(X=x_1,x_2,....\)</span>），将随机变量取这些值的概率一一列举出来的概率分布就是该随机变量的概率分布。对于离散型随机变量，它的概率分布可以用枚举法表示<span class="arithmatex">\(P(X=x_i)\)</span>；对于连续型随机变量，它的概率分布可以用<span class="arithmatex">\(P(X)\)</span>表示。</p>
<p>-----------<font color=00ff00>以上两个概念都是概念性的东西，并没有公式</font>---------------</p>
<h3 id="3">3、概率密度函数</h3>
<p>设<span class="arithmatex">\(X\)</span>为一<strong>连续型随机变量</strong>（对于连续性随机变量，它为某一具体数值的概率为 0，只能用范围<span class="arithmatex">\([x_1, x_2]\)</span>来表示它的概率），<span class="arithmatex">\(x\)</span>为任意实数，<span class="arithmatex">\(X\)</span>的概率密度函数记为<span class="arithmatex">\(f(x)\)</span>,概率密度函数满足条件：
（1）<span class="arithmatex">\(f(x) \ge 0\)</span>；（2）<span class="arithmatex">\(\int_{+\infty}^{-\infty}f(x)dx=1\)</span>
<strong>概率密度函数f(x)和概率p(x)的关系：对于任何实数<span class="arithmatex">\(x_1&lt;x_2\)</span></strong>, <span class="arithmatex">\(p(x_1&lt;x&lt;x_2)\)</span>是该概率密度曲线下从<span class="arithmatex">\(x_1\)</span>到<span class="arithmatex">\(x_2\)</span>的面积：<span class="arithmatex">\(p(x_1&lt;x&lt;x_2)=\int_{x_1}^{x_2}f(x)dx\)</span></p>
<h3 id="4cumulative-distribution-function">4、分布函数（也叫累积分布函数，cumulative distribution function）</h3>
<p>连续型随机变量的概率<strong>也可以</strong>用分布函数<span class="arithmatex">\(F(x)\)</span>来表示，分布函数的定义为：
<span class="arithmatex">\(<span class="arithmatex">\(F(x)=p(X\le x)=\int_{-\infty}^{x}f(t)dt\)</span>\)</span>其中，<span class="arithmatex">\(-\infty &lt; x &lt; + \infty\)</span>。分布函数是随机变量最重要的概率特征，分布函数可以完整地描述随机变量的统计规律。
<strong><font color=ff00>分布函数和概率密度函数的关系：概率密度函数就是变量X在某个很小的取值区间[<span class="arithmatex">\(x_1\)</span>, <span class="arithmatex">\(x_2\)</span>]（因为连续性变量取某个确定的值的概率为0）中的概率所表示的函数，概率分布就是变量X在某个大的曲直区间[<span class="arithmatex">\(-\infty\)</span>, <span class="arithmatex">\(x_1\)</span>]中的概率，所以概率分布就是概率密度函数的累加，也就是说，对于连续型随机变量，概率密度函数是分布函数的导数，而分布函数是概率密度函数从负无穷到指定点的积分。‌‌物理上来讲，分布函数是概率密度函数曲线下小于<span class="arithmatex">\(x\)</span>的面积</font></strong></p>
<p><img alt="中图示例" src="https://images.unsplash.com/photo-1464822759023-fed622ff2c3b?w=600&amp;h=400&amp;fit=crop#pic_center" /></p>
<h3 id="5">5、均值和方差</h3>
<p>凡是随机变量符合一定的分布，就有均值和方差。所以如果确定了一个分布的方差和均值，这个分布就100%确定了。
<strong>均值（mean）</strong>：对于一个随机变量<span class="arithmatex">\(X\)</span>,均值也叫<span class="arithmatex">\(X\)</span>的期望值（denoted by <span class="arithmatex">\(E(X)\)</span>），即所有样本的平均值。
对于离散随机变量 <span class="arithmatex">\(X\)</span> 来说，它的均值等于 <span class="arithmatex">\(\sum xP(x)\)</span>,其中，<span class="arithmatex">\(x\)</span>代表随机变量 <span class="arithmatex">\(X\)</span> 的所有可能的取值（对于离散型随机变量来说，<span class="arithmatex">\(P(x)\)</span>也叫概率质量函数）。
对于连续随机变量 <span class="arithmatex">\(X\)</span> 来说，它的均值等于<span class="arithmatex">\(\int_{-\infty }^{+\infty}xP(x)dx\)</span>，这里的<span class="arithmatex">\(f(x)\)</span>是随机变量 <span class="arithmatex">\(X\)</span>的概率密度函数。</p>
<p><strong>方差（variance）</strong>：方差等于每个样本点与均值之差的平方和除以样本数量。</p>
<p>对于一个离散变量<span class="arithmatex">\(X\)</span>（一般数据的频次是给定的）：
<strong>均值</strong>：<span class="arithmatex">\(E(X)=\frac{\sum^{n}_{i=1}x_i}{n}\)</span>，<span class="arithmatex">\(x_i\)</span>是离散变量<span class="arithmatex">\(X\)</span>的所有取值，<span class="arithmatex">\(n\)</span>是数据的个数，这个公式表示把所有数据加起来，然后除以数据的个数，得到的就是均值。
<strong>方差</strong>：<span class="arithmatex">\(S^2=\frac{\sum^{n}_{i=1}(x_i-E(X))^2}{n}\)</span>,这个公式表示先求出每个数据与均值的差，然后平方，再把这些平方值加起来，最后除以数据的个数，得到的就是方差。</p>
<h3 id="6normal-distribution"><font color=ffzz> 6、正态分布（normal distribution）</font></h3>
<p>正态分布也称高斯分布，通常记作<span class="arithmatex">\(X～N(μ ,σ^2)\)</span>，其中<span class="arithmatex">\(\mu\)</span>称为均值，<span class="arithmatex">\(\sigma^2\)</span>称为方差。正态分布的概率密度函数（说明是连续的）<span class="arithmatex">\(f(x)\)</span>符合如下式：
<span class="arithmatex">\(<span class="arithmatex">\(f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)</span>\)</span>
<span class="arithmatex">\(μ = 0\)</span>,<span class="arithmatex">\(σ = 1\)</span>的正态分布被称为标准正态分布</p>
<h3 id="6likelihood-function"><font color=ffzz>6、似然函数likelihood function（似然）</font></h3>
<p>wiki：似然函数（也叫似然）指的是一个统计模型用来解释观察到的数据的好坏情况的评判，通过计算在不同模型元素值的情况下看到某些数据的概率来比较的。换句话来讲，我们前面讲的概率密度函数<span class="arithmatex">\(f(x)\)</span>其实都是在分布固定的情况下写的（即模型的参数<span class="arithmatex">\(\theta\)</span>是固定的），按理来讲应该写成<span class="arithmatex">\(f(x|\theta)\)</span>（一般来讲，<span class="arithmatex">\(f(x|\theta)\)</span>这样的表达都会避免，而是用<span class="arithmatex">\(f(x;\theta)\)</span>或者<span class="arithmatex">\(f(x, \theta)\)</span>，这样做是为了暗示<span class="arithmatex">\(\theta\)</span>是一个固定的未知数而不是有条件的随机变量.），如果反过来，当取值<span class="arithmatex">\(x\)</span>是固定的时候，<span class="arithmatex">\(f(x|\theta)\)</span>就变成了似然函数了，为了避免歧义，一般将似然函数写成：<span class="arithmatex">\(\mathcal{L}(\theta|x)\)</span>。关于参数<span class="arithmatex">\(θ\)</span>的似然函数<span class="arithmatex">\(L(θ|x)\)</span>（在数值上）等于给定参数<span class="arithmatex">\(θ\)</span>后变量<span class="arithmatex">\(X\)</span>的概率：<span class="arithmatex">\(L(\theta|x)=P(X|x=\theta)\)</span>。似然函数在推断统计学（Statistical inference）中扮演重要角色，尤其是在参数估计方法中。在教科书中，似然常常被用作“概率”的同义词。但是在统计学中，二者有截然不同的用法。概率描述了已知参数时的随机变量的输出结果；似然则用来描述已知随机变量输出结果时，未知参数的可能取值。</p>
<h3 id="7kl">7、KL散度</h3>
<p>KL散度是信息论里面的知识点。通俗解释就是用一个分布P来表示另一个分布Q而减少的信息熵。
<span class="arithmatex">\(KL(Q||P)\)</span>表示用分布Q来表示分布P对所减少的信息熵，<span class="arithmatex">\(KL(P||Q)\)</span>表示用分布P来表示分布Q对所减少的信息熵，所以<span class="arithmatex">\(KL(Q||P)\neq KL(P||Q)\)</span>。</p>
<p><strong><font color=ff0000>step into what truly need to learn formally, lets get started!</font></strong></p>
<h1 id="ae">AE</h1>
<p>autoencoder 的思路和原理比较简单，输入数据input_data通过encoder 压缩到特定维度，之后通过 decoder 恢复成原始维度，然后直接计算 decoder 后的数据与input_data 之间的MSE损失，以此来达到无监督提取特征的功能。（实现方式特别暴力！！！）</p>
<h1 id="vae">VAE</h1>
<p>VAE 模型是AE的变体，相比于AE直接粗暴的用encoder压缩输入数据的特征的到隐变量然后通过decoder建模重建的特征，VAE做了进一步延伸，将隐变量的概率分布引入到建模过程中，它的encoder不像AE那样直接输出固定的隐变量矩阵，而且矩阵的值是 discrete，而是输出一个多变量的高斯分布（说明学到了输入数据的均值和方差），说明学习到的值是continuous，这也是AVE比AE好的原因。通过学习输入数据的分布来学习数据的特征。
<font color=ff0000>VAE work flow:</font>
假定实验数据集是MNIST（1 <span class="arithmatex">\(\times\)</span> 28 <span class="arithmatex">\(\times\)</span> 28），隐空间维度是 8，batch_size 是128， 所以输入数据就是[128， 784]，通过 encoder 后得到一个代表输入数据均值 <span class="arithmatex">\(\mu\)</span> 的矩阵[128, 8]和一个代表输入数据标准差 <span class="arithmatex">\(\theta\)</span> 的矩阵[128, 8]，基于这两个矩阵可以得到一个 8 变量的高斯分布（这就是encoder的输出），之后从这个 8 变量的高斯分布中采样的到一个[128, 8]的数据，经过encoder进行解码恢复得到output，即[128， 784]的矩阵。<strong>这个简单操作背后的数学原理</strong>：首先encoder生成的概率模型是基于 p(z|x) 建模的，它的到的是 p(z|x) 分布的均值和方差，然后 decoder 的过程是基于 q(x|z) 建模的，</p>
<p>这个代表prior：<span class="arithmatex">\(p(z)\)</span>，之后从这个 8 变量的高斯分布中采样的到一个[128, 8]的数据，经过encoder进行解码恢复得到[128， 784]的矩阵，这个矩阵就是代表posterior：<span class="arithmatex">\(p(z|x)\)</span>，换句话说，就是In other words, the value of observable variables x, given a value for latent variable z.</p>
<p>假如输入的图像特征是<span class="arithmatex">\(X\)</span>（这个<span class="arithmatex">\(X\)</span>代表很多特征点的集合，可以理解为随机变量），<span class="arithmatex">\(X\)</span>符合分布<span class="arithmatex">\(P(X)\)</span>，它的分布函数为<span class="arithmatex">\(P(x)\)</span>，VAE的目的就是通过估计出这个分布（即通过 VAE 模型的参数计算出来的特征符合<span class="arithmatex">\(P(X)\)</span>）。首先，<span class="arithmatex">\(P(X)\)</span>可以看作是各种高斯混合分布的叠加（任意的数据分布，都可以由若干的<span class="arithmatex">\(m\)</span>个高斯分布组成），公式表示如下：
<span class="arithmatex">\(<span class="arithmatex">\(P(X)=\sum P(z)P(x|z)\)</span>\)</span> <span class="arithmatex">\(x\)</span>表示随机变量<span class="arithmatex">\(X\)</span>的取值，<span class="arithmatex">\(P(z)\)</span>是第<span class="arithmatex">\(z\)</span>个高斯分布，<span class="arithmatex">\(P(x|z)\)</span>是取到第<span class="arithmatex">\(m\)</span>个高斯分布的概率（或权重）。</p>
<p>上面的情况表示的是有限个高斯分布的（离散）形式，但现实情况下，<span class="arithmatex">\(P(X)\)</span>通常是由无限个的高斯组成的，因此，将其变成积分的形式，代表无限个高斯分布的组合。上式可以写为：
<span class="arithmatex">\(<span class="arithmatex">\(P(x)=\int_{z}P(z)P(x|z)dz\)</span>\)</span>
其中，<span class="arithmatex">\(P(z)\)</span>表示高斯分布，<span class="arithmatex">\(z\sim N(0,1)\)</span>，而<span class="arithmatex">\(P(x|z)\)</span>是未知的，可以理解为条件分布（在高斯分布<span class="arithmatex">\(z\)</span>出现的情况下，特征<span class="arithmatex">\(x\)</span>出现的概率满足的分布），它的均值和方差为，<span class="arithmatex">\(x|z \sim N(\mu(z), \sigma(z))\)</span>。于是我们真正需要求解的就是<span class="arithmatex">\(\mu(z)\)</span>和<span class="arithmatex">\(\sigma(z)\)</span>的表达式，所以VAE引入encoder和decoder来求解。
第一个神经网络叫做Encoder，它求解的结果是<span class="arithmatex">\(q(z|x)\)</span>, <span class="arithmatex">\(q(x)\)</span>可以代表任何分布。第二个神经网络叫做Decoder，它求解的是<span class="arithmatex">\(\mu(z)\)</span>和<span class="arithmatex">\(\sigma(z)\)</span>，等价于求解<span class="arithmatex">\(P(x|z)\)</span>，其中，第一个神经网络Encoder的目的就是辅助第一个Decoder求解<span class="arithmatex">\(P(x|z)\)</span>。</p>
<p><font color=ff0000>VAE的工作原理更深入:</font>
上面讲到，模型的目的就是估计输入图像的特征（随机变量）<span class="arithmatex">\(X\)</span>的分布<span class="arithmatex">\(P(X)\)</span>，然后通过该分布生成符合该分布的新图像出来。由上面的介绍可知，分布函数<span class="arithmatex">\(P(x)\)</span>表示的是随机变量<span class="arithmatex">\(X\)</span>的取值小于某一数值<span class="arithmatex">\(x\)</span>的概率，我们希望学习尽可能多的数据，即随机变量<span class="arithmatex">\(X\)</span>的取值范围足够大，所以<span class="arithmatex">\(X\)</span>的分布函数<span class="arithmatex">\(P(x)\)</span>也越大，这等价于求解：
<span class="arithmatex">\(<span class="arithmatex">\(maximum L=\sum_{x}logP(x)\)</span>\)</span>给定任意一个分布<span class="arithmatex">\(q(x)\)</span>,有<span class="arithmatex">\(\int_{z}q(z)q(x|z)dz=1\)</span>，于是根据贝叶斯定理可以推导出：
<span class="arithmatex">\(<span class="arithmatex">\(\begin{align}logP(x)=log\int_{z}q(z|x)P(x)dz\\=\int_{z}q(z|x)log\frac{P(z,x)}{P(z|x)}dz\\
=\int_{z}q(z|x)log\frac{P(z,x)}{q(z|x)}\cdot\frac{q(z|x)}{P(z|x)}dz\\=
\int_{z}q(z|x)log\frac{P(z,x)}{q(z|x)}dz+\int_{z}q(z|x)log\frac{q(z|x)}{P(z|x)}dz\\=\int_{z}q(z|x)log\frac{P(z,x)}{q(z|x)}dz+KL(q(z|x)||P(z|x))\end{align}\)</span>\)</span>
<span class="arithmatex">\((5)\)</span>式中右边第二项为<span class="arithmatex">\(q(z|x)\)</span>和<span class="arithmatex">\(P(z|x)\)</span>两个分布之间的KL散度距离，根据KL公式性质，该项恒大于等于0，记：
<span class="arithmatex">\(<span class="arithmatex">\(\begin{align}L_b=\int_{z}q(z|x)log\frac{P(z,x)}{q(z|x)}dz=\int_{z}q(z|x)log\frac{P(x|z)\cdot P(z)}{q(z|x)}dz\end{align}\)</span>\)</span>于是可以得到：
<span class="arithmatex">\(<span class="arithmatex">\(\begin{align}logP(x)=L_b + KL(q(z|x)||P(z|x))\ge L_b\end{align}\)</span>\)</span></p>
<p>原来是要求使得<span class="arithmatex">\(logP(x)\)</span>最大化的<span class="arithmatex">\(P(x∣z)\)</span>，现在引入了一个<span class="arithmatex">\(q(z|x)\)</span>，就转换成了同时求<span class="arithmatex">\(P(x∣z)\)</span>和<span class="arithmatex">\(q(z|x)\)</span>使得<span class="arithmatex">\(logP(x)\)</span>最大化,不妨来观察下<span class="arithmatex">\(logP(x)\)</span>和<span class="arithmatex">\(L_b\)</span>的关系：
根据公式<span class="arithmatex">\(P(x)=\int_zP(z)P(x|z)dz\)</span>，当我们固定住<span class="arithmatex">\(P(x|z)\)</span>时，由于<span class="arithmatex">\(P(z)\)</span>是固定的，所以<span class="arithmatex">\(P(x)\)</span>也是固定的，那么<span class="arithmatex">\(logP(x)\)</span>也就固定住了。而根据 <span class="arithmatex">\(6\)</span> 式可知，如果调节<span class="arithmatex">\(q(z|x)\)</span>使得分布<span class="arithmatex">\(q(z|x)\)</span>与分布<span class="arithmatex">\(P(z|x)\)</span>接近相等，即<span class="arithmatex">\(L_b\)</span>越来越大，如果分布<span class="arithmatex">\(q(z|x)\)</span>与分布<span class="arithmatex">\(P(z|x)\)</span>完全相等，即KL散度为0时，此时<span class="arithmatex">\(L_b\)</span>就完全等同于<span class="arithmatex">\(logP(x)\)</span>。所以，无论<span class="arithmatex">\(logP(x)\)</span>的值如何，总可以通过调节<span class="arithmatex">\(q(z|x)\)</span>使得<span class="arithmatex">\(L_b\)</span>等于它，又因为<span class="arithmatex">\(L_b\)</span>是<span class="arithmatex">\(logP(x)\)</span>的下界，所以求解<span class="arithmatex">\(max  logP(x)\)</span>等价于求解：
<span class="arithmatex">\(<span class="arithmatex">\(\begin{align} maximum (L_b) \end{align}\)</span>\)</span>
从宏观角度来看，调节<span class="arithmatex">\(P(z|x)\)</span>就是相当于调节Decoder，调节<span class="arithmatex">\(q(z|x)\)</span>就是调节Encoder，于是VAE模型的算法便是：Decoder每改进一次，Encoder就调节成跟其一致，并且利用约束项迫使Decoder在训练的时候“只能前进，不能后退”。这便是VAE的巧妙设计之处。</p>
<p>现在只需求解<span class="arithmatex">\(maximum (L_b)\)</span>，注意到：
<span class="arithmatex">\(<span class="arithmatex">\(\begin{align}L_b=\int_{z}q(z|x)log\frac{P(x|z)\cdot P(z)}{q(z|x)}dz\\=\int_{z}q(z|x)log\frac{P(z)}{q(z|x)}dz+\int_{z}q(z|x)log{P(x|z)}dz\\=-KL(q(z|x)||P(z))+\int_{z}q(z|x)log{P(x|z)}dz
\end{align}\)</span>\)</span>
所以，求解<span class="arithmatex">\(maximum (L_b)\)</span>等价于求解KL的最小值和<span class="arithmatex">\(q(z|x)log{P(x|z)\)</span>的最大值，第一项的展开式等于：
<span class="arithmatex">\(<span class="arithmatex">\(\begin{align}\sum_{i=1}^{j}exp(\sigma_i)-(1+\sigma_i)+(m_i)^2\end{align}\)</span>\)</span>
于是，第一项式子就是第二节VAE模型架构中第二个损失函数的由来。</p>
<p>第二项注意到:</p>
<p><span class="arithmatex">\(<span class="arithmatex">\(\begin{align} maxmum \int_{z}q(z|x)logP(x|z)dz\\=maxmum E_{q(z|x)}[P(x|z)] \end{align}\)</span>\)</span>
上述的这个期望，也就是表明在给定<span class="arithmatex">\(q(z|x)\)</span>的情况下，<span class="arithmatex">\(P(x|z)\)</span>的值尽可能高，第二项式子就是第二节VAE模型架构中第一个损失函数的由来。</p>








                    </div>
                    <!-- end of page-inner -->
                </div>
                <!-- end of page-wrapper -->
            </div>
            <!-- end of body-inner -->
        </div>
        <!-- end of book-body -->
        <script src="../js/main.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
        <script src="../javascripts/highlight_init.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/cpp.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
        <script src="../javascripts/mathjax_config.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="../javascripts/my_posts.js"></script>
        <script src="../javascripts/recode_life.js"></script>
        <script src="../search/main.js"></script>
        <script src="../js/gitbook.min.js"></script>
        <script src="../js/theme.min.js"></script>
</body>

</html>




